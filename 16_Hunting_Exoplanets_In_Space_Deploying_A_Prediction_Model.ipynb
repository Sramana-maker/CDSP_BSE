{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VcKiuR0gIvqi",
        "F3PeA6-AaO5J",
        "lP_x1sHBbWce",
        "CSKFSFb5gZge",
        "DIi2Gikph5HQ",
        "Xz8BPprpipOq",
        "Bd5jKRKZo7am",
        "Rh9l4z6r7oM0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsEYRKMIXz6"
      },
      "source": [
        "# Lesson 16: Hunting Exoplanets In Space - Deploying A Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCJZ1yNT2-sw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRurfsok2_bZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fb-qEJ63Fxr"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wx-ukah3I0-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmZPtYSn7kA"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the last class, we learnt how to create line plots and scatter plots to visualise data. \n",
        "\n",
        "In this class, we will deploy a **prediction model** to predict which stars in the test dataset have a planet and which do not. \n",
        "\n",
        "**How prediction model works?**\n",
        "\n",
        "**1. Learning:**\n",
        "- The prediction model, through the training dataset, will learn the properties of a star that has a planet and also the properties of a star which does not have a planet. \n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-galaxy-more+planet-apt-c16-01.png\" height=400/>\n",
        "\n",
        "\n",
        "**2. Testing and Prediction:**  \n",
        "\n",
        "- Once the model has learnt the required properties, it will look for these properties in the test dataset and according to the properties it sees, it will predict whether a star has a planet or not.\n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-galaxy-less+planet-apt-c16-01.png\" height=400/>\n",
        "\n",
        "---\n",
        "\n",
        "In this class, we will learn how to deploy the **Random Forest Classifier** model (a machine learning algorithm). \n",
        "\n",
        "**What is Machine Learning?**\n",
        "\n",
        "- Machine learning is a branch of artificial intelligence in which a machine learns through data the different features on its own without being programmed by a computer programmer. \n",
        "\n",
        "```\n",
        "TEACHER\n",
        "    Without machine learning, we need to tell the machine what to do. \n",
        "```\n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-comp-boy-apt-c16.gif\" height=400 >\n",
        "\n",
        "\n",
        "\n",
        "With machine learning, the machine learning will figure out what to do itself, using the inputs we feed it.\n",
        "    \n",
        "- First, the algorithmic models are **trained** to perform some specific tasks by learning patterns from the data it sees, rather than through computer programming by a human expert.\n",
        "\n",
        "- After training, our model is ready for making predictions for any new input.\n",
        "\n",
        "```\n",
        "TEACHER\n",
        "   In this image, you can see that the machine is learning from the data \n",
        "   provided. for example, it has learned how a cup looks like from the alphabet \n",
        "   chart. Now, whenever the machine sees a cup again, it can easily recognize \n",
        "   that it is a cup.\n",
        "\n",
        "```\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-machine-tarining-apt-c16-01.png\" />\n",
        "\n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-comp-tea+cup-apt-c16-01.png\" />\n",
        "\n",
        "\n",
        "\n",
        "- For our model, the machine will learn to recognise the flux values of stars having a planet on its own.\n",
        "-  When a new dataset containing only the flux values of a star is shown to the machine, it will tell whether that star has a planet or not.\n",
        "\n",
        "There are many machine learning models or algorithms to do this kind of prediction. One of them is **Random Forest Classifier**. \n",
        "\n",
        "---\n",
        "**Random Forest Classifier:**\n",
        "\n",
        "- It is used to classify outcomes into classes (or labels) based on some features. \n",
        "- For e.g., an animal that makes a 'Meow! Meow!' sound is classified (or labelled) as a cat, an animal that makes a 'Woof! Woof!' sound is classified as a dog, an animal which makes a hissing sound is classified as a snake etc.\n",
        "- It uses the training data to learn how the given features relate to a particular class. \n",
        "- **For example:** A classifier can classify a given set of population into following two classes:\n",
        "  - Those suffering from Diabetes.\n",
        "  - Those not suffering from Diabetes.\n",
        "\n",
        "  on the basis of different features of the population such as age, gender, blood pressure etc. \n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-pateint-database-apt-c16-01.png\" height=400/>\n",
        "\n",
        "In this lesson, we will use the Random Forest Classifier model to classify whether a star has a planet or not. \n",
        "- The stars which have at least one planet are labelled as `2` \n",
        "- The stars not having a planet are labelled as `1`. \n",
        "\n",
        "\n",
        "Let's run all the codes in the code cells that we have already covered in the previous classes and begin this class from **Activity 1: The `value_counts()` Function** section.  You too run the code cells until the first activity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq9mAtEg9cpZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwzryU6Zie33"
      },
      "source": [
        "#### Loading The Datasets\n",
        "\n",
        "Create a Pandas DataFrame every time you start a Jupyter notebook.\n",
        "\n",
        "Dataset links (Don't click on them):\n",
        "\n",
        "1. Train dataset \n",
        "\n",
        "   \n",
        "2. Test dataset \n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjScdqmQstFg"
      },
      "source": [
        "# Loading both the training and test datasets.\n",
        "import pandas as pd\n",
        "\n",
        "exo_train_df = pd.read_csv('')\n",
        "exo_test_df = pd.read_csv('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aGcwb3Ax10u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd6a88b-a86c-43b8-eddd-8ef33ac2de19"
      },
      "source": [
        "# Number of rows and columns in the DataFrames.\n",
        "print(exo_train_df.shape)\n",
        "exo_test_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5087, 3198)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(570, 3198)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VG6uuL7eq3f"
      },
      "source": [
        "In the previous classes, we have already checked that the training dataset does not have any missing value. So, we can skip the missing values check part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_UHtcNaE-s"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKiuR0gIvqi"
      },
      "source": [
        "#### Activity 1: The `value_counts()` Function\n",
        "Our prediction model should classify the stars either as `1` or `2`. Let's find out how many stars in the test dataset are classified as `1` and `2`.\n",
        "\n",
        "To compute how many times a value occurs in a series, we use the `value_counts()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG6oVSvRK6Jh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4916be1e-e103-4460-d13a-86fd3fbf64c9"
      },
      "source": [
        "# Student Action: Count the number of times a value occurs in a Pandas series.\n",
        "exo_test_df['LABEL'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    565\n",
              "2      5\n",
              "Name: LABEL, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l7BTsS7LNLE"
      },
      "source": [
        "There are `565` stars which are classified as `1` and `5` stars classified as `2` which means only `5` stars have a planet. Interestingly, if our prediction model mindlessly classifies every star as `1`, then it is a very accurate model. Why?\n",
        "\n",
        "Because the accuracy of a model is calculated as **a percentage of the correct predictions out of the total number of predictions**. In this case, the percentage of the correct predictions is\n",
        "\n",
        " $\\frac{565\\times100}{570} = 99.122$ %\n",
        "\n",
        "Thus, without actually deploying a proper prediction model, we can predict the stars having a planet with 99% accuracy. \n",
        "\n",
        "This is **WRONG**! This is where we need to be careful. Because we have very imbalanced data. The ultimate goal of the Kepler space telescope is to detect exoplanets in outer space. Hence, a machine learning model, based on some data should also **correctly** detect stars having planets. This means a prediction model will be considered useful if it correctly detects almost all the stars having a planet.\n",
        "\n",
        "So, the prediction model which always labels every star as `1` is useless. Because it must detect almost all the stars having a planet.\n",
        "\n",
        "Now, we are going to deploy the Random Forest Classifier model so that it can detect all the five (or at least three) stars having a planet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmlkMPBed0wi"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbbP1ucnjism"
      },
      "source": [
        "### Random Forest Classifier^ \n",
        "\n",
        "A Random Forest is a collection (a.k.a. ensemble) of many decision trees. A decision tree is a flow chart which separates data based on some condition. If a condition is true, you move on a path otherwise, you move on to another path.\n",
        "\n",
        "\n",
        "For e.g., in case of finding a star having a planet, you can construct the following decision tree: \n",
        "\n",
        "<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-14/decision-tree.png' width=600>\n",
        "\n",
        "You could ask a question whether there is decrease in the flux values of a star. If the answer is no, then it clearly means the star does not have a planet. However, if the answer is yes, then you could ask another question to check whether the decrease is periodic or not. Again, if the answer is no, then the star does not have a planet. Otherwise, it has a planet.\n",
        "\n",
        "This is one of the examples of a decision tree. Based on a problem, the decision tree could get more and more complex. \n",
        "\n",
        "A collection of `N` number of trees is a random forest wherein each tree gives some predicted value (in this case either class `1` or class `2`). \n",
        "\n",
        "\n",
        "<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-14/rfc-image.jpg' width=800>\n",
        "\n",
        "<h3><b>For example:</h3></b>\n",
        "\n",
        "```\n",
        "TEACHER\n",
        "   For example, a random forest model is used to classify whether an item is a \n",
        "   pen or a pencil. There are 3 decision trees, each predicting a class for the \n",
        "   item. The final predicted value is the majority class, which in this case is \n",
        "   Class 2 i.e. Pen.\n",
        "```\n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/Untitled-13-01.png\" height=400>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The final predicted value is the majority class, i.e, the class that is predicted by the most number of decision trees in a random forest.\n",
        "\n",
        "For the time being, just consider the Random Forest Classifier as some kind of a black-box which classifies data into different classes (in this case, either class `1` or class `2`) by learning the properties of every class through a training dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zr_egypdy1R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3PeA6-AaO5J"
      },
      "source": [
        "#### Activity 2: Importing `RandomForestClassifier`\n",
        "\n",
        "We need to import a module called `RandomForestClassifier` from a package called `sklearn.ensemble`. The `sklearn` (or **scikit-learn**) is a collection of many machine learning modules. Almost every machine learning algorithm can be directly applied without a knowledge of math using the **scikit-learn** library. It is kind of a plug-and-play device.\n",
        "\n",
        "You can read about it in the link provided in the **Activities** section under the title **`scikit-learn` - Random Forest Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjyPky3Hh0fu"
      },
      "source": [
        "# Teacher Action: Import the required modules from the 'sklearn' library.\n",
        "# Import the 'RandomForestClassifier' module from the 'sklearn.ensemble' library.\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8FljiKddvz9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP_x1sHBbWce"
      },
      "source": [
        "#### Activity 3: Target & Feature Variables Separation\n",
        "\n",
        "The `RandomForestClassifier` module has a function called `fit()` which takes two inputs: \n",
        "1. The first input is the collection of feature variables.\n",
        "2. The second input is the target variable.  \n",
        "\n",
        "Let us try to understand what are Feature variables and Target Variables.\n",
        "\n",
        "**Features and Target:**\n",
        "\n",
        "<h3><font color=red><b>\n",
        "Have you ever came across any facial recognition system? <img src=\"https://curriculum.whitehatjr.com/APT+Asset/Final+C2+images/question.jpg\" height=25/></b></h3>\n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/whj-face-recognition-apt-c16-01.png\"/>\n",
        "\n",
        "\n",
        "Face recognition is a biometric identification process which is used to authenticate a person using its **facial features**. The system examines various facial **features** such as eyes, nose and lips to authenticate a target user.  \n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/eyenoselip.png\" height=400/>\n",
        "\n",
        "Similarly, the machine learning algorithm uses various features to predict the outcome. \n",
        "\n",
        "```\n",
        "NOTE for the TEACHER:\n",
        "   The teacher can also give following example if the student is still\n",
        "   not able to understand the concept of features and target. \n",
        "\n",
        "  Example: An algorithm may analyse different features of an apple like size, \n",
        "  color, taste etc to uniquely identify an apple. \n",
        "  In this case, \n",
        "  features --> size, color, taste\n",
        "  target   --> 'apple' or 'not apple'\n",
        "```\n",
        "\n",
        "\n",
        "For our dataset, we will use flux values of a star i.e.`FLUX.1` to `FLUX.3197` columns to find out whether that star has a planet or not.\n",
        "\n",
        " \n",
        "**Feature Variables:** The features are those variables which describe the features or properties of an entity.\n",
        "\n",
        "**Target variable:** The variable which needs to be predicted is called a target variable.\n",
        "\n",
        "\n",
        "\n",
        "For our dataset, \n",
        "- `FLUX.1` to `FLUX.3197` are feature variables. Hence, the values stored in these columns are the features of a star in exoplanets dataset.\n",
        "- `LABEL` is the target variable because the prediction model needs to predict which star belongs to which class in the test dataset. Hence, the values stored in the `LABEL` column are the target values. \n",
        "\n",
        "<img src=\"https://curriculum.whitehatjr.com/APT+Asset/APT+C16/WHJ-feature+variable-target-apt-c16-01.png\"/>\n",
        "\n",
        "So, we need to extract the target variable and the feature variables separately from the training dataset. \n",
        "\n",
        "Let's store the feature variables in the `x_train` variable and the target variable in the `y_train` variable. We will separate the features using the `iloc[]` function. \n",
        "\n",
        "Recall the syntax for `iloc[]` function.\n",
        "\n",
        "**Syntax:** \n",
        "\n",
        "`dataframe_name.iloc[row_position_start : row_position_end, column_position_start : column_position_end]`\n",
        "\n",
        "\n",
        "We need all the rows from the training set. So, inside the `iloc[]` function, we will enter the colon (`:`) sign to get all the rows. We do not need the first column, i.e., the `LABEL` column. Therefore, inside the `iloc[]` function, as part of column indexing, enter `1` as the starting index followed by the colon (`:`) sign to include the rest of the columns from the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4xbNQzYj_CM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f61fe49b-0b2c-41d2-8757-8475190bb2cf"
      },
      "source": [
        "# Student Action: Extract the feature variables from the training dataset using the 'iloc[]' function.\n",
        "x_train = exo_train_df.iloc[:, 1:]\n",
        "x_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FLUX.1</th>\n",
              "      <th>FLUX.2</th>\n",
              "      <th>FLUX.3</th>\n",
              "      <th>FLUX.4</th>\n",
              "      <th>FLUX.5</th>\n",
              "      <th>FLUX.6</th>\n",
              "      <th>FLUX.7</th>\n",
              "      <th>FLUX.8</th>\n",
              "      <th>FLUX.9</th>\n",
              "      <th>FLUX.10</th>\n",
              "      <th>FLUX.11</th>\n",
              "      <th>FLUX.12</th>\n",
              "      <th>FLUX.13</th>\n",
              "      <th>FLUX.14</th>\n",
              "      <th>FLUX.15</th>\n",
              "      <th>FLUX.16</th>\n",
              "      <th>FLUX.17</th>\n",
              "      <th>FLUX.18</th>\n",
              "      <th>FLUX.19</th>\n",
              "      <th>FLUX.20</th>\n",
              "      <th>FLUX.21</th>\n",
              "      <th>FLUX.22</th>\n",
              "      <th>FLUX.23</th>\n",
              "      <th>FLUX.24</th>\n",
              "      <th>FLUX.25</th>\n",
              "      <th>FLUX.26</th>\n",
              "      <th>FLUX.27</th>\n",
              "      <th>FLUX.28</th>\n",
              "      <th>FLUX.29</th>\n",
              "      <th>FLUX.30</th>\n",
              "      <th>FLUX.31</th>\n",
              "      <th>FLUX.32</th>\n",
              "      <th>FLUX.33</th>\n",
              "      <th>FLUX.34</th>\n",
              "      <th>FLUX.35</th>\n",
              "      <th>FLUX.36</th>\n",
              "      <th>FLUX.37</th>\n",
              "      <th>FLUX.38</th>\n",
              "      <th>FLUX.39</th>\n",
              "      <th>FLUX.40</th>\n",
              "      <th>...</th>\n",
              "      <th>FLUX.3158</th>\n",
              "      <th>FLUX.3159</th>\n",
              "      <th>FLUX.3160</th>\n",
              "      <th>FLUX.3161</th>\n",
              "      <th>FLUX.3162</th>\n",
              "      <th>FLUX.3163</th>\n",
              "      <th>FLUX.3164</th>\n",
              "      <th>FLUX.3165</th>\n",
              "      <th>FLUX.3166</th>\n",
              "      <th>FLUX.3167</th>\n",
              "      <th>FLUX.3168</th>\n",
              "      <th>FLUX.3169</th>\n",
              "      <th>FLUX.3170</th>\n",
              "      <th>FLUX.3171</th>\n",
              "      <th>FLUX.3172</th>\n",
              "      <th>FLUX.3173</th>\n",
              "      <th>FLUX.3174</th>\n",
              "      <th>FLUX.3175</th>\n",
              "      <th>FLUX.3176</th>\n",
              "      <th>FLUX.3177</th>\n",
              "      <th>FLUX.3178</th>\n",
              "      <th>FLUX.3179</th>\n",
              "      <th>FLUX.3180</th>\n",
              "      <th>FLUX.3181</th>\n",
              "      <th>FLUX.3182</th>\n",
              "      <th>FLUX.3183</th>\n",
              "      <th>FLUX.3184</th>\n",
              "      <th>FLUX.3185</th>\n",
              "      <th>FLUX.3186</th>\n",
              "      <th>FLUX.3187</th>\n",
              "      <th>FLUX.3188</th>\n",
              "      <th>FLUX.3189</th>\n",
              "      <th>FLUX.3190</th>\n",
              "      <th>FLUX.3191</th>\n",
              "      <th>FLUX.3192</th>\n",
              "      <th>FLUX.3193</th>\n",
              "      <th>FLUX.3194</th>\n",
              "      <th>FLUX.3195</th>\n",
              "      <th>FLUX.3196</th>\n",
              "      <th>FLUX.3197</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.85</td>\n",
              "      <td>83.81</td>\n",
              "      <td>20.10</td>\n",
              "      <td>-26.98</td>\n",
              "      <td>-39.56</td>\n",
              "      <td>-124.71</td>\n",
              "      <td>-135.18</td>\n",
              "      <td>-96.27</td>\n",
              "      <td>-79.89</td>\n",
              "      <td>-160.17</td>\n",
              "      <td>-207.47</td>\n",
              "      <td>-154.88</td>\n",
              "      <td>-173.71</td>\n",
              "      <td>-146.56</td>\n",
              "      <td>-120.26</td>\n",
              "      <td>-102.85</td>\n",
              "      <td>-98.71</td>\n",
              "      <td>-48.42</td>\n",
              "      <td>-86.57</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>-25.85</td>\n",
              "      <td>-67.39</td>\n",
              "      <td>-36.55</td>\n",
              "      <td>-87.01</td>\n",
              "      <td>-97.72</td>\n",
              "      <td>-131.59</td>\n",
              "      <td>-134.80</td>\n",
              "      <td>-186.97</td>\n",
              "      <td>-244.32</td>\n",
              "      <td>-225.76</td>\n",
              "      <td>-229.60</td>\n",
              "      <td>-253.48</td>\n",
              "      <td>-145.74</td>\n",
              "      <td>-145.74</td>\n",
              "      <td>30.47</td>\n",
              "      <td>-173.39</td>\n",
              "      <td>-187.56</td>\n",
              "      <td>-192.88</td>\n",
              "      <td>-182.76</td>\n",
              "      <td>-195.99</td>\n",
              "      <td>...</td>\n",
              "      <td>-167.69</td>\n",
              "      <td>-56.86</td>\n",
              "      <td>7.56</td>\n",
              "      <td>37.40</td>\n",
              "      <td>-81.13</td>\n",
              "      <td>-20.10</td>\n",
              "      <td>-30.34</td>\n",
              "      <td>-320.48</td>\n",
              "      <td>-320.48</td>\n",
              "      <td>-287.72</td>\n",
              "      <td>-351.25</td>\n",
              "      <td>-70.07</td>\n",
              "      <td>-194.34</td>\n",
              "      <td>-106.47</td>\n",
              "      <td>-14.80</td>\n",
              "      <td>63.13</td>\n",
              "      <td>130.03</td>\n",
              "      <td>76.43</td>\n",
              "      <td>131.90</td>\n",
              "      <td>-193.16</td>\n",
              "      <td>-193.16</td>\n",
              "      <td>-89.26</td>\n",
              "      <td>-17.56</td>\n",
              "      <td>-17.31</td>\n",
              "      <td>125.62</td>\n",
              "      <td>68.87</td>\n",
              "      <td>100.01</td>\n",
              "      <td>-9.60</td>\n",
              "      <td>-25.39</td>\n",
              "      <td>-16.51</td>\n",
              "      <td>-78.07</td>\n",
              "      <td>-102.15</td>\n",
              "      <td>-102.15</td>\n",
              "      <td>25.13</td>\n",
              "      <td>48.57</td>\n",
              "      <td>92.54</td>\n",
              "      <td>39.32</td>\n",
              "      <td>61.42</td>\n",
              "      <td>5.08</td>\n",
              "      <td>-39.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-38.88</td>\n",
              "      <td>-33.83</td>\n",
              "      <td>-58.54</td>\n",
              "      <td>-40.09</td>\n",
              "      <td>-79.31</td>\n",
              "      <td>-72.81</td>\n",
              "      <td>-86.55</td>\n",
              "      <td>-85.33</td>\n",
              "      <td>-83.97</td>\n",
              "      <td>-73.38</td>\n",
              "      <td>-86.51</td>\n",
              "      <td>-74.97</td>\n",
              "      <td>-73.15</td>\n",
              "      <td>-86.13</td>\n",
              "      <td>-76.57</td>\n",
              "      <td>-61.27</td>\n",
              "      <td>-37.23</td>\n",
              "      <td>-48.53</td>\n",
              "      <td>-30.96</td>\n",
              "      <td>-8.14</td>\n",
              "      <td>-5.54</td>\n",
              "      <td>15.79</td>\n",
              "      <td>45.71</td>\n",
              "      <td>10.61</td>\n",
              "      <td>40.66</td>\n",
              "      <td>16.70</td>\n",
              "      <td>15.18</td>\n",
              "      <td>11.98</td>\n",
              "      <td>-203.70</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>17.02</td>\n",
              "      <td>-8.50</td>\n",
              "      <td>-13.87</td>\n",
              "      <td>-29.10</td>\n",
              "      <td>-34.29</td>\n",
              "      <td>-24.68</td>\n",
              "      <td>...</td>\n",
              "      <td>-36.75</td>\n",
              "      <td>-15.49</td>\n",
              "      <td>-13.24</td>\n",
              "      <td>20.46</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>27.80</td>\n",
              "      <td>-58.20</td>\n",
              "      <td>-58.20</td>\n",
              "      <td>-72.04</td>\n",
              "      <td>-58.01</td>\n",
              "      <td>-30.92</td>\n",
              "      <td>-13.42</td>\n",
              "      <td>-13.98</td>\n",
              "      <td>-5.43</td>\n",
              "      <td>8.71</td>\n",
              "      <td>1.80</td>\n",
              "      <td>36.59</td>\n",
              "      <td>-9.80</td>\n",
              "      <td>-19.53</td>\n",
              "      <td>-19.53</td>\n",
              "      <td>-24.32</td>\n",
              "      <td>-23.88</td>\n",
              "      <td>-33.07</td>\n",
              "      <td>-9.03</td>\n",
              "      <td>3.75</td>\n",
              "      <td>11.61</td>\n",
              "      <td>-12.66</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>12.53</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-24.89</td>\n",
              "      <td>-4.86</td>\n",
              "      <td>0.76</td>\n",
              "      <td>-11.70</td>\n",
              "      <td>6.46</td>\n",
              "      <td>16.00</td>\n",
              "      <td>19.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>532.64</td>\n",
              "      <td>535.92</td>\n",
              "      <td>513.73</td>\n",
              "      <td>496.92</td>\n",
              "      <td>456.45</td>\n",
              "      <td>466.00</td>\n",
              "      <td>464.50</td>\n",
              "      <td>486.39</td>\n",
              "      <td>436.56</td>\n",
              "      <td>484.39</td>\n",
              "      <td>469.66</td>\n",
              "      <td>462.30</td>\n",
              "      <td>492.23</td>\n",
              "      <td>441.20</td>\n",
              "      <td>483.17</td>\n",
              "      <td>481.28</td>\n",
              "      <td>535.31</td>\n",
              "      <td>554.34</td>\n",
              "      <td>562.80</td>\n",
              "      <td>540.14</td>\n",
              "      <td>576.34</td>\n",
              "      <td>551.67</td>\n",
              "      <td>556.69</td>\n",
              "      <td>550.86</td>\n",
              "      <td>577.33</td>\n",
              "      <td>562.08</td>\n",
              "      <td>577.97</td>\n",
              "      <td>530.67</td>\n",
              "      <td>553.27</td>\n",
              "      <td>538.33</td>\n",
              "      <td>527.17</td>\n",
              "      <td>532.50</td>\n",
              "      <td>273.66</td>\n",
              "      <td>273.66</td>\n",
              "      <td>292.39</td>\n",
              "      <td>298.44</td>\n",
              "      <td>252.64</td>\n",
              "      <td>233.58</td>\n",
              "      <td>171.41</td>\n",
              "      <td>224.02</td>\n",
              "      <td>...</td>\n",
              "      <td>-51.09</td>\n",
              "      <td>-33.30</td>\n",
              "      <td>-61.53</td>\n",
              "      <td>-89.61</td>\n",
              "      <td>-69.17</td>\n",
              "      <td>-86.47</td>\n",
              "      <td>-140.91</td>\n",
              "      <td>-84.20</td>\n",
              "      <td>-84.20</td>\n",
              "      <td>-89.09</td>\n",
              "      <td>-55.44</td>\n",
              "      <td>-61.05</td>\n",
              "      <td>-29.17</td>\n",
              "      <td>-63.80</td>\n",
              "      <td>-57.61</td>\n",
              "      <td>2.70</td>\n",
              "      <td>-31.25</td>\n",
              "      <td>-47.09</td>\n",
              "      <td>-6.53</td>\n",
              "      <td>14.00</td>\n",
              "      <td>14.00</td>\n",
              "      <td>-25.05</td>\n",
              "      <td>-34.98</td>\n",
              "      <td>-32.08</td>\n",
              "      <td>-17.06</td>\n",
              "      <td>-27.77</td>\n",
              "      <td>7.86</td>\n",
              "      <td>-70.77</td>\n",
              "      <td>-64.44</td>\n",
              "      <td>-83.83</td>\n",
              "      <td>-71.69</td>\n",
              "      <td>13.31</td>\n",
              "      <td>13.31</td>\n",
              "      <td>-29.89</td>\n",
              "      <td>-20.88</td>\n",
              "      <td>5.06</td>\n",
              "      <td>-11.80</td>\n",
              "      <td>-28.91</td>\n",
              "      <td>-70.02</td>\n",
              "      <td>-96.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>326.52</td>\n",
              "      <td>347.39</td>\n",
              "      <td>302.35</td>\n",
              "      <td>298.13</td>\n",
              "      <td>317.74</td>\n",
              "      <td>312.70</td>\n",
              "      <td>322.33</td>\n",
              "      <td>311.31</td>\n",
              "      <td>312.42</td>\n",
              "      <td>323.33</td>\n",
              "      <td>311.14</td>\n",
              "      <td>326.19</td>\n",
              "      <td>313.11</td>\n",
              "      <td>313.89</td>\n",
              "      <td>317.96</td>\n",
              "      <td>330.92</td>\n",
              "      <td>341.10</td>\n",
              "      <td>360.58</td>\n",
              "      <td>370.29</td>\n",
              "      <td>369.71</td>\n",
              "      <td>339.00</td>\n",
              "      <td>336.24</td>\n",
              "      <td>319.31</td>\n",
              "      <td>321.56</td>\n",
              "      <td>308.02</td>\n",
              "      <td>296.82</td>\n",
              "      <td>279.34</td>\n",
              "      <td>275.78</td>\n",
              "      <td>289.67</td>\n",
              "      <td>281.33</td>\n",
              "      <td>285.37</td>\n",
              "      <td>281.87</td>\n",
              "      <td>88.75</td>\n",
              "      <td>88.75</td>\n",
              "      <td>67.71</td>\n",
              "      <td>74.46</td>\n",
              "      <td>69.34</td>\n",
              "      <td>76.51</td>\n",
              "      <td>80.26</td>\n",
              "      <td>70.31</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.75</td>\n",
              "      <td>14.29</td>\n",
              "      <td>-14.18</td>\n",
              "      <td>-25.14</td>\n",
              "      <td>-13.43</td>\n",
              "      <td>-14.74</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-31.07</td>\n",
              "      <td>-31.07</td>\n",
              "      <td>-50.27</td>\n",
              "      <td>-39.22</td>\n",
              "      <td>-51.33</td>\n",
              "      <td>-18.53</td>\n",
              "      <td>-1.99</td>\n",
              "      <td>10.43</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-15.32</td>\n",
              "      <td>-23.38</td>\n",
              "      <td>-27.71</td>\n",
              "      <td>-36.12</td>\n",
              "      <td>-36.12</td>\n",
              "      <td>-15.65</td>\n",
              "      <td>6.63</td>\n",
              "      <td>10.66</td>\n",
              "      <td>-8.57</td>\n",
              "      <td>-8.29</td>\n",
              "      <td>-21.90</td>\n",
              "      <td>-25.80</td>\n",
              "      <td>-29.86</td>\n",
              "      <td>7.42</td>\n",
              "      <td>5.71</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>30.05</td>\n",
              "      <td>20.03</td>\n",
              "      <td>-12.67</td>\n",
              "      <td>-8.77</td>\n",
              "      <td>-17.31</td>\n",
              "      <td>-17.35</td>\n",
              "      <td>13.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1107.21</td>\n",
              "      <td>-1112.59</td>\n",
              "      <td>-1118.95</td>\n",
              "      <td>-1095.10</td>\n",
              "      <td>-1057.55</td>\n",
              "      <td>-1034.48</td>\n",
              "      <td>-998.34</td>\n",
              "      <td>-1022.71</td>\n",
              "      <td>-989.57</td>\n",
              "      <td>-970.88</td>\n",
              "      <td>-933.30</td>\n",
              "      <td>-889.49</td>\n",
              "      <td>-888.66</td>\n",
              "      <td>-853.95</td>\n",
              "      <td>-800.91</td>\n",
              "      <td>-754.48</td>\n",
              "      <td>-717.24</td>\n",
              "      <td>-649.34</td>\n",
              "      <td>-605.71</td>\n",
              "      <td>-575.62</td>\n",
              "      <td>-526.37</td>\n",
              "      <td>-490.12</td>\n",
              "      <td>-458.73</td>\n",
              "      <td>-447.76</td>\n",
              "      <td>-419.54</td>\n",
              "      <td>-410.76</td>\n",
              "      <td>-404.10</td>\n",
              "      <td>-425.38</td>\n",
              "      <td>-397.29</td>\n",
              "      <td>-412.73</td>\n",
              "      <td>-446.49</td>\n",
              "      <td>-413.46</td>\n",
              "      <td>-1006.21</td>\n",
              "      <td>-1006.21</td>\n",
              "      <td>-973.29</td>\n",
              "      <td>-986.01</td>\n",
              "      <td>-975.88</td>\n",
              "      <td>-982.20</td>\n",
              "      <td>-953.73</td>\n",
              "      <td>-964.35</td>\n",
              "      <td>...</td>\n",
              "      <td>-694.76</td>\n",
              "      <td>-705.01</td>\n",
              "      <td>-625.24</td>\n",
              "      <td>-604.16</td>\n",
              "      <td>-668.26</td>\n",
              "      <td>-742.18</td>\n",
              "      <td>-820.55</td>\n",
              "      <td>-874.76</td>\n",
              "      <td>-874.76</td>\n",
              "      <td>-853.68</td>\n",
              "      <td>-808.62</td>\n",
              "      <td>-777.88</td>\n",
              "      <td>-712.62</td>\n",
              "      <td>-694.01</td>\n",
              "      <td>-655.74</td>\n",
              "      <td>-599.74</td>\n",
              "      <td>-617.30</td>\n",
              "      <td>-602.98</td>\n",
              "      <td>-539.29</td>\n",
              "      <td>-672.71</td>\n",
              "      <td>-672.71</td>\n",
              "      <td>-594.49</td>\n",
              "      <td>-597.60</td>\n",
              "      <td>-560.77</td>\n",
              "      <td>-501.95</td>\n",
              "      <td>-461.62</td>\n",
              "      <td>-468.59</td>\n",
              "      <td>-513.24</td>\n",
              "      <td>-504.70</td>\n",
              "      <td>-521.95</td>\n",
              "      <td>-594.37</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-357.24</td>\n",
              "      <td>-443.76</td>\n",
              "      <td>-438.54</td>\n",
              "      <td>-399.71</td>\n",
              "      <td>-384.65</td>\n",
              "      <td>-411.79</td>\n",
              "      <td>-510.54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3197 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    FLUX.1   FLUX.2   FLUX.3  ...  FLUX.3195  FLUX.3196  FLUX.3197\n",
              "0    93.85    83.81    20.10  ...      61.42       5.08     -39.54\n",
              "1   -38.88   -33.83   -58.54  ...       6.46      16.00      19.93\n",
              "2   532.64   535.92   513.73  ...     -28.91     -70.02     -96.67\n",
              "3   326.52   347.39   302.35  ...     -17.31     -17.35      13.98\n",
              "4 -1107.21 -1112.59 -1118.95  ...    -384.65    -411.79    -510.54\n",
              "\n",
              "[5 rows x 3197 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYlFRpMUoX0o"
      },
      "source": [
        "Now, let's get only the target variables from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3bOVajkEos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431df8c4-58d5-42ad-cc48-5738d03f71d0"
      },
      "source": [
        "# Student Action: Using the 'iloc[]' function, retrieve only the first column, i.e., the 'LABEL' column from the training dataset.\n",
        "y_train = exo_train_df.iloc[:, 0] \n",
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "Name: LABEL, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4c2YLzWdtiC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSKFSFb5gZge"
      },
      "source": [
        "#### Activity 4: Fitting The Model\n",
        "\n",
        "Now that we have separated the feature and target variables for deploying the `RandomForestClassifier` model, let's train the model with the feature variables using the `fit()` function. The steps to be followed are described below.\n",
        "\n",
        "1. First, call the `RandomForestClassifier` module with inputs as `n_jobs=-1` and `n_estimators=50`. Store the function in a variable with the name `rf_clf`.\n",
        "\n",
        "  For the time being, ignore the reason behind providing the `n_jobs=-1` parameter as an input.\n",
        "\n",
        "  ```python\n",
        "  rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=50)\n",
        "  ```\n",
        "\n",
        "  The `n_estimators` parameter defines the number of decision trees in a Random Forest. Therefore, `n_estimators=50` means that the forest contains `50` decision trees. If `n_estimators` parameter is not defined by a user, then by default, the forest contains `100` decision trees.\n",
        "\n",
        "2. Call the `fit()` function with `x_train` and `y_train` as inputs.\n",
        "\n",
        "  ```python\n",
        "  rf_clf.fit(x_train, y_train)\n",
        "  ```\n",
        "3. Call the `score()` function with `x_train` and `y_train` as inputs to check the accuracy score of the model. This step is actually not required. If you wish, you can skip this step.\n",
        "  \n",
        "  ```\n",
        "  rf_clf.score(x_train, y_train)\n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAU1jEBVkHG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8a7001-712c-4c06-fccd-f736aa9cf81d"
      },
      "source": [
        "# Teacher Action: Train the 'RandomForestClassifier' model using the 'fit()' function.\n",
        "\n",
        "# 1. First, call the 'RandomForestClassifier' module with inputs as 'n_jobs = - 1' & 'n_estimators=50'. Store it in a variable with the name 'rf_clf'.\n",
        "# For the time being, ignore the reason behind providing the 'n_jobs=-1' parameter as an input.\n",
        "rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=50)\n",
        "\n",
        "# 2. Call the 'fit()' function with 'x_train' and 'y_train' as inputs.\n",
        "rf_clf.fit(x_train, y_train)\n",
        "\n",
        "# 3. Call the 'score()' function with 'x_train' and 'y_train' as inputs to check the accuracy score of the model.\n",
        "rf_clf.score(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9998034204835856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKq3HlbxhU0k"
      },
      "source": [
        "As you can see, we have deployed the `RandomForestClassifier` model with an accuracy of 100% (the number `1.0` signifies 100% accuracy).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV57P4UFh1rt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIi2Gikph5HQ"
      },
      "source": [
        "#### Activity 5: Target & Feature Variables From Test Dataset^^\n",
        "\n",
        "Now we need to make predictions on the test dataset. So, we just need to extract feature variables from the test dataset `exo_test_df` using the `iloc[]` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN1nsa2qpr80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e4562ec7-d7d6-4e68-e693-fa1d09d0708e"
      },
      "source": [
        "# Student Action: Using the 'iloc[]' function, extract the feature variables from the test dataset.\n",
        "x_test = exo_test_df.iloc[:, 1:]\n",
        "x_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FLUX.1</th>\n",
              "      <th>FLUX.2</th>\n",
              "      <th>FLUX.3</th>\n",
              "      <th>FLUX.4</th>\n",
              "      <th>FLUX.5</th>\n",
              "      <th>FLUX.6</th>\n",
              "      <th>FLUX.7</th>\n",
              "      <th>FLUX.8</th>\n",
              "      <th>FLUX.9</th>\n",
              "      <th>FLUX.10</th>\n",
              "      <th>FLUX.11</th>\n",
              "      <th>FLUX.12</th>\n",
              "      <th>FLUX.13</th>\n",
              "      <th>FLUX.14</th>\n",
              "      <th>FLUX.15</th>\n",
              "      <th>FLUX.16</th>\n",
              "      <th>FLUX.17</th>\n",
              "      <th>FLUX.18</th>\n",
              "      <th>FLUX.19</th>\n",
              "      <th>FLUX.20</th>\n",
              "      <th>FLUX.21</th>\n",
              "      <th>FLUX.22</th>\n",
              "      <th>FLUX.23</th>\n",
              "      <th>FLUX.24</th>\n",
              "      <th>FLUX.25</th>\n",
              "      <th>FLUX.26</th>\n",
              "      <th>FLUX.27</th>\n",
              "      <th>FLUX.28</th>\n",
              "      <th>FLUX.29</th>\n",
              "      <th>FLUX.30</th>\n",
              "      <th>FLUX.31</th>\n",
              "      <th>FLUX.32</th>\n",
              "      <th>FLUX.33</th>\n",
              "      <th>FLUX.34</th>\n",
              "      <th>FLUX.35</th>\n",
              "      <th>FLUX.36</th>\n",
              "      <th>FLUX.37</th>\n",
              "      <th>FLUX.38</th>\n",
              "      <th>FLUX.39</th>\n",
              "      <th>FLUX.40</th>\n",
              "      <th>...</th>\n",
              "      <th>FLUX.3158</th>\n",
              "      <th>FLUX.3159</th>\n",
              "      <th>FLUX.3160</th>\n",
              "      <th>FLUX.3161</th>\n",
              "      <th>FLUX.3162</th>\n",
              "      <th>FLUX.3163</th>\n",
              "      <th>FLUX.3164</th>\n",
              "      <th>FLUX.3165</th>\n",
              "      <th>FLUX.3166</th>\n",
              "      <th>FLUX.3167</th>\n",
              "      <th>FLUX.3168</th>\n",
              "      <th>FLUX.3169</th>\n",
              "      <th>FLUX.3170</th>\n",
              "      <th>FLUX.3171</th>\n",
              "      <th>FLUX.3172</th>\n",
              "      <th>FLUX.3173</th>\n",
              "      <th>FLUX.3174</th>\n",
              "      <th>FLUX.3175</th>\n",
              "      <th>FLUX.3176</th>\n",
              "      <th>FLUX.3177</th>\n",
              "      <th>FLUX.3178</th>\n",
              "      <th>FLUX.3179</th>\n",
              "      <th>FLUX.3180</th>\n",
              "      <th>FLUX.3181</th>\n",
              "      <th>FLUX.3182</th>\n",
              "      <th>FLUX.3183</th>\n",
              "      <th>FLUX.3184</th>\n",
              "      <th>FLUX.3185</th>\n",
              "      <th>FLUX.3186</th>\n",
              "      <th>FLUX.3187</th>\n",
              "      <th>FLUX.3188</th>\n",
              "      <th>FLUX.3189</th>\n",
              "      <th>FLUX.3190</th>\n",
              "      <th>FLUX.3191</th>\n",
              "      <th>FLUX.3192</th>\n",
              "      <th>FLUX.3193</th>\n",
              "      <th>FLUX.3194</th>\n",
              "      <th>FLUX.3195</th>\n",
              "      <th>FLUX.3196</th>\n",
              "      <th>FLUX.3197</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>119.88</td>\n",
              "      <td>100.21</td>\n",
              "      <td>86.46</td>\n",
              "      <td>48.68</td>\n",
              "      <td>46.12</td>\n",
              "      <td>39.39</td>\n",
              "      <td>18.57</td>\n",
              "      <td>6.98</td>\n",
              "      <td>6.63</td>\n",
              "      <td>-21.97</td>\n",
              "      <td>-23.17</td>\n",
              "      <td>-29.26</td>\n",
              "      <td>-33.99</td>\n",
              "      <td>-6.25</td>\n",
              "      <td>-28.12</td>\n",
              "      <td>-27.24</td>\n",
              "      <td>-32.28</td>\n",
              "      <td>-12.29</td>\n",
              "      <td>-16.57</td>\n",
              "      <td>-23.86</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>9.24</td>\n",
              "      <td>35.52</td>\n",
              "      <td>81.20</td>\n",
              "      <td>116.49</td>\n",
              "      <td>133.99</td>\n",
              "      <td>148.97</td>\n",
              "      <td>174.15</td>\n",
              "      <td>187.77</td>\n",
              "      <td>215.30</td>\n",
              "      <td>246.80</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-52.05</td>\n",
              "      <td>-31.52</td>\n",
              "      <td>-31.15</td>\n",
              "      <td>-48.53</td>\n",
              "      <td>-38.93</td>\n",
              "      <td>-26.06</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>12.26</td>\n",
              "      <td>-7.06</td>\n",
              "      <td>-23.53</td>\n",
              "      <td>2.54</td>\n",
              "      <td>30.21</td>\n",
              "      <td>38.87</td>\n",
              "      <td>-22.86</td>\n",
              "      <td>-22.86</td>\n",
              "      <td>-4.37</td>\n",
              "      <td>2.27</td>\n",
              "      <td>-16.27</td>\n",
              "      <td>-30.84</td>\n",
              "      <td>-7.21</td>\n",
              "      <td>-4.27</td>\n",
              "      <td>13.60</td>\n",
              "      <td>15.62</td>\n",
              "      <td>31.96</td>\n",
              "      <td>49.89</td>\n",
              "      <td>86.93</td>\n",
              "      <td>86.93</td>\n",
              "      <td>42.99</td>\n",
              "      <td>48.76</td>\n",
              "      <td>22.82</td>\n",
              "      <td>32.79</td>\n",
              "      <td>30.76</td>\n",
              "      <td>14.55</td>\n",
              "      <td>10.92</td>\n",
              "      <td>22.68</td>\n",
              "      <td>5.91</td>\n",
              "      <td>14.52</td>\n",
              "      <td>19.29</td>\n",
              "      <td>14.44</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>13.33</td>\n",
              "      <td>45.50</td>\n",
              "      <td>31.93</td>\n",
              "      <td>35.78</td>\n",
              "      <td>269.43</td>\n",
              "      <td>57.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5736.59</td>\n",
              "      <td>5699.98</td>\n",
              "      <td>5717.16</td>\n",
              "      <td>5692.73</td>\n",
              "      <td>5663.83</td>\n",
              "      <td>5631.16</td>\n",
              "      <td>5626.39</td>\n",
              "      <td>5569.47</td>\n",
              "      <td>5550.44</td>\n",
              "      <td>5458.80</td>\n",
              "      <td>5329.39</td>\n",
              "      <td>5191.38</td>\n",
              "      <td>5031.39</td>\n",
              "      <td>4769.89</td>\n",
              "      <td>4419.66</td>\n",
              "      <td>4218.92</td>\n",
              "      <td>3924.73</td>\n",
              "      <td>3605.30</td>\n",
              "      <td>3326.55</td>\n",
              "      <td>3021.20</td>\n",
              "      <td>2800.61</td>\n",
              "      <td>2474.48</td>\n",
              "      <td>2258.33</td>\n",
              "      <td>1951.69</td>\n",
              "      <td>1749.86</td>\n",
              "      <td>1585.38</td>\n",
              "      <td>1575.48</td>\n",
              "      <td>1568.41</td>\n",
              "      <td>1661.08</td>\n",
              "      <td>1977.33</td>\n",
              "      <td>2425.62</td>\n",
              "      <td>2889.61</td>\n",
              "      <td>3847.64</td>\n",
              "      <td>3847.64</td>\n",
              "      <td>3741.20</td>\n",
              "      <td>3453.47</td>\n",
              "      <td>3202.61</td>\n",
              "      <td>2923.73</td>\n",
              "      <td>2694.84</td>\n",
              "      <td>2474.22</td>\n",
              "      <td>...</td>\n",
              "      <td>-3470.75</td>\n",
              "      <td>-4510.72</td>\n",
              "      <td>-5013.41</td>\n",
              "      <td>-3636.05</td>\n",
              "      <td>-2324.27</td>\n",
              "      <td>-2688.55</td>\n",
              "      <td>-2813.66</td>\n",
              "      <td>-586.22</td>\n",
              "      <td>-586.22</td>\n",
              "      <td>-756.80</td>\n",
              "      <td>-1090.23</td>\n",
              "      <td>-1388.61</td>\n",
              "      <td>-1745.36</td>\n",
              "      <td>-2015.28</td>\n",
              "      <td>-2359.06</td>\n",
              "      <td>-2516.66</td>\n",
              "      <td>-2699.31</td>\n",
              "      <td>-2777.55</td>\n",
              "      <td>-2732.97</td>\n",
              "      <td>1167.39</td>\n",
              "      <td>1167.39</td>\n",
              "      <td>1368.89</td>\n",
              "      <td>1434.80</td>\n",
              "      <td>1360.75</td>\n",
              "      <td>1148.44</td>\n",
              "      <td>1117.67</td>\n",
              "      <td>714.86</td>\n",
              "      <td>419.02</td>\n",
              "      <td>57.06</td>\n",
              "      <td>-175.66</td>\n",
              "      <td>-581.91</td>\n",
              "      <td>-984.09</td>\n",
              "      <td>-1230.89</td>\n",
              "      <td>-1600.45</td>\n",
              "      <td>-1824.53</td>\n",
              "      <td>-2061.17</td>\n",
              "      <td>-2265.98</td>\n",
              "      <td>-2366.19</td>\n",
              "      <td>-2294.86</td>\n",
              "      <td>-2034.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>844.48</td>\n",
              "      <td>817.49</td>\n",
              "      <td>770.07</td>\n",
              "      <td>675.01</td>\n",
              "      <td>605.52</td>\n",
              "      <td>499.45</td>\n",
              "      <td>440.77</td>\n",
              "      <td>362.95</td>\n",
              "      <td>207.27</td>\n",
              "      <td>150.46</td>\n",
              "      <td>85.49</td>\n",
              "      <td>-20.12</td>\n",
              "      <td>-35.88</td>\n",
              "      <td>-65.59</td>\n",
              "      <td>-15.12</td>\n",
              "      <td>16.60</td>\n",
              "      <td>-25.70</td>\n",
              "      <td>61.88</td>\n",
              "      <td>53.18</td>\n",
              "      <td>64.32</td>\n",
              "      <td>72.38</td>\n",
              "      <td>100.35</td>\n",
              "      <td>67.26</td>\n",
              "      <td>14.71</td>\n",
              "      <td>-16.41</td>\n",
              "      <td>-147.46</td>\n",
              "      <td>-231.27</td>\n",
              "      <td>-320.29</td>\n",
              "      <td>-407.82</td>\n",
              "      <td>-450.48</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-166.30</td>\n",
              "      <td>-139.90</td>\n",
              "      <td>-96.41</td>\n",
              "      <td>-23.49</td>\n",
              "      <td>13.59</td>\n",
              "      <td>67.59</td>\n",
              "      <td>...</td>\n",
              "      <td>-35.24</td>\n",
              "      <td>-70.13</td>\n",
              "      <td>-35.30</td>\n",
              "      <td>-56.48</td>\n",
              "      <td>-74.60</td>\n",
              "      <td>-115.18</td>\n",
              "      <td>-8.91</td>\n",
              "      <td>-37.59</td>\n",
              "      <td>-37.59</td>\n",
              "      <td>-37.43</td>\n",
              "      <td>-104.23</td>\n",
              "      <td>-101.45</td>\n",
              "      <td>-107.35</td>\n",
              "      <td>-109.82</td>\n",
              "      <td>-126.27</td>\n",
              "      <td>-170.32</td>\n",
              "      <td>-117.85</td>\n",
              "      <td>-32.30</td>\n",
              "      <td>-70.18</td>\n",
              "      <td>314.29</td>\n",
              "      <td>314.29</td>\n",
              "      <td>314.29</td>\n",
              "      <td>149.71</td>\n",
              "      <td>54.60</td>\n",
              "      <td>12.60</td>\n",
              "      <td>-133.68</td>\n",
              "      <td>-78.16</td>\n",
              "      <td>-52.30</td>\n",
              "      <td>-8.55</td>\n",
              "      <td>-19.73</td>\n",
              "      <td>17.82</td>\n",
              "      <td>-51.66</td>\n",
              "      <td>-48.29</td>\n",
              "      <td>-59.99</td>\n",
              "      <td>-82.10</td>\n",
              "      <td>-174.54</td>\n",
              "      <td>-95.23</td>\n",
              "      <td>-162.68</td>\n",
              "      <td>-36.79</td>\n",
              "      <td>30.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-826.00</td>\n",
              "      <td>-827.31</td>\n",
              "      <td>-846.12</td>\n",
              "      <td>-836.03</td>\n",
              "      <td>-745.50</td>\n",
              "      <td>-784.69</td>\n",
              "      <td>-791.22</td>\n",
              "      <td>-746.50</td>\n",
              "      <td>-709.53</td>\n",
              "      <td>-679.56</td>\n",
              "      <td>-706.03</td>\n",
              "      <td>-720.56</td>\n",
              "      <td>-631.12</td>\n",
              "      <td>-659.16</td>\n",
              "      <td>-672.03</td>\n",
              "      <td>-665.06</td>\n",
              "      <td>-667.94</td>\n",
              "      <td>-660.84</td>\n",
              "      <td>-672.75</td>\n",
              "      <td>-644.91</td>\n",
              "      <td>-680.53</td>\n",
              "      <td>-620.50</td>\n",
              "      <td>-570.34</td>\n",
              "      <td>-530.00</td>\n",
              "      <td>-537.88</td>\n",
              "      <td>-578.38</td>\n",
              "      <td>-532.34</td>\n",
              "      <td>-532.38</td>\n",
              "      <td>-491.03</td>\n",
              "      <td>-485.03</td>\n",
              "      <td>-427.19</td>\n",
              "      <td>-380.84</td>\n",
              "      <td>-329.50</td>\n",
              "      <td>-286.91</td>\n",
              "      <td>-283.81</td>\n",
              "      <td>-298.19</td>\n",
              "      <td>-271.03</td>\n",
              "      <td>-268.50</td>\n",
              "      <td>-209.56</td>\n",
              "      <td>-180.44</td>\n",
              "      <td>...</td>\n",
              "      <td>16.50</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-14.94</td>\n",
              "      <td>64.09</td>\n",
              "      <td>8.38</td>\n",
              "      <td>45.31</td>\n",
              "      <td>100.72</td>\n",
              "      <td>91.53</td>\n",
              "      <td>46.69</td>\n",
              "      <td>20.34</td>\n",
              "      <td>30.94</td>\n",
              "      <td>-36.81</td>\n",
              "      <td>-33.28</td>\n",
              "      <td>-69.62</td>\n",
              "      <td>-208.00</td>\n",
              "      <td>-280.28</td>\n",
              "      <td>-340.41</td>\n",
              "      <td>-337.41</td>\n",
              "      <td>-268.03</td>\n",
              "      <td>-245.00</td>\n",
              "      <td>-230.62</td>\n",
              "      <td>-129.59</td>\n",
              "      <td>-35.47</td>\n",
              "      <td>122.34</td>\n",
              "      <td>93.03</td>\n",
              "      <td>93.03</td>\n",
              "      <td>68.81</td>\n",
              "      <td>9.81</td>\n",
              "      <td>20.75</td>\n",
              "      <td>20.25</td>\n",
              "      <td>-120.81</td>\n",
              "      <td>-257.56</td>\n",
              "      <td>-215.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-39.57</td>\n",
              "      <td>-15.88</td>\n",
              "      <td>-9.16</td>\n",
              "      <td>-6.37</td>\n",
              "      <td>-16.13</td>\n",
              "      <td>-24.05</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-45.20</td>\n",
              "      <td>-5.04</td>\n",
              "      <td>14.62</td>\n",
              "      <td>-19.52</td>\n",
              "      <td>-11.43</td>\n",
              "      <td>-49.80</td>\n",
              "      <td>25.84</td>\n",
              "      <td>11.62</td>\n",
              "      <td>3.18</td>\n",
              "      <td>-9.59</td>\n",
              "      <td>14.49</td>\n",
              "      <td>8.82</td>\n",
              "      <td>32.32</td>\n",
              "      <td>-28.90</td>\n",
              "      <td>-28.90</td>\n",
              "      <td>-14.09</td>\n",
              "      <td>-30.87</td>\n",
              "      <td>-18.99</td>\n",
              "      <td>-38.60</td>\n",
              "      <td>-27.79</td>\n",
              "      <td>9.65</td>\n",
              "      <td>29.60</td>\n",
              "      <td>7.88</td>\n",
              "      <td>42.87</td>\n",
              "      <td>27.59</td>\n",
              "      <td>27.05</td>\n",
              "      <td>20.26</td>\n",
              "      <td>29.48</td>\n",
              "      <td>9.71</td>\n",
              "      <td>22.84</td>\n",
              "      <td>25.99</td>\n",
              "      <td>-667.55</td>\n",
              "      <td>-1336.24</td>\n",
              "      <td>...</td>\n",
              "      <td>-122.12</td>\n",
              "      <td>-32.01</td>\n",
              "      <td>-47.15</td>\n",
              "      <td>-56.45</td>\n",
              "      <td>-41.71</td>\n",
              "      <td>-34.13</td>\n",
              "      <td>-43.12</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-24.29</td>\n",
              "      <td>22.29</td>\n",
              "      <td>25.18</td>\n",
              "      <td>1.84</td>\n",
              "      <td>-22.29</td>\n",
              "      <td>-26.43</td>\n",
              "      <td>-12.12</td>\n",
              "      <td>-33.05</td>\n",
              "      <td>-21.66</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-187.35</td>\n",
              "      <td>-166.23</td>\n",
              "      <td>-115.54</td>\n",
              "      <td>-50.18</td>\n",
              "      <td>-37.96</td>\n",
              "      <td>-22.37</td>\n",
              "      <td>-4.74</td>\n",
              "      <td>-35.82</td>\n",
              "      <td>-37.87</td>\n",
              "      <td>-61.85</td>\n",
              "      <td>-27.15</td>\n",
              "      <td>-21.18</td>\n",
              "      <td>-33.76</td>\n",
              "      <td>-85.34</td>\n",
              "      <td>-81.46</td>\n",
              "      <td>-61.98</td>\n",
              "      <td>-69.34</td>\n",
              "      <td>-17.84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3197 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    FLUX.1   FLUX.2   FLUX.3  ...  FLUX.3195  FLUX.3196  FLUX.3197\n",
              "0   119.88   100.21    86.46  ...      35.78     269.43      57.72\n",
              "1  5736.59  5699.98  5717.16  ...   -2366.19   -2294.86   -2034.72\n",
              "2   844.48   817.49   770.07  ...    -162.68     -36.79      30.63\n",
              "3  -826.00  -827.31  -846.12  ...    -120.81    -257.56    -215.41\n",
              "4   -39.57   -15.88    -9.16  ...     -61.98     -69.34     -17.84\n",
              "\n",
              "[5 rows x 3197 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsZe8fyeiXBG"
      },
      "source": [
        "Let's also extract the target variable from the test dataset `exo_test_df` so that we can compare the actual target values with the predicted values later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVMq86Y5p5ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86563a46-976b-44d0-808c-b803585ca43b"
      },
      "source": [
        "# Student Action: Using the 'iloc[]' function, extract the target variable from the test dataset.\n",
        "y_test = exo_test_df.iloc[:, 0]\n",
        "y_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "Name: LABEL, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3OVTo4ioQk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz8BPprpipOq"
      },
      "source": [
        "#### Activity 6: The `predict()` Function^^^\n",
        "\n",
        "Now, let's make predictions on the test dataset by calling the `predict()` function with the features variables of the test dataset as an input.\n",
        "\n",
        "**Syntax of predict() function:**\n",
        "\n",
        "```python\n",
        "model.predict(data)\n",
        "```\n",
        " where, `data` is a set of feature variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF_lb5Fcp8zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408aa07a-690f-43ba-c4d4-2b8e40c938ba"
      },
      "source": [
        "# Student Action: Make predictions on the test dataset by using the 'predict()' function.\n",
        "y_predicted = rf_clf.predict(x_test)\n",
        "y_predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pKzHPiEjCSI"
      },
      "source": [
        "The `predict()` function returns a NumPy array of the predicted values. You can verify it using the `type()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOz8o4fJp-_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8320ab5-2181-44b9-c473-50442c2df0c7"
      },
      "source": [
        "# Student Action: Using the 'type()' function, verify that the predicted values are obtained in the form of a NumPy array.\n",
        "type(y_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhdBmNx8jWQv"
      },
      "source": [
        "The actual target values are stored in a Pandas series. So, for the sake of consistency, let's convert the NumPy array of the predicted values into a Pandas series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF3skQa0qBeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a4d5c5-0113-4dfe-ded6-4372fd74d72f"
      },
      "source": [
        "# Student Action: Convert the NumPy array of predicted values into a Pandas series.\n",
        "y_predicted = pd.Series(y_predicted)\n",
        "y_predicted.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    1\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQwrexkrjrdE"
      },
      "source": [
        "Now, let's count the number of stars classified as `1` and `2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk7UnroAqTUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c3be7c-641f-431c-95c6-d7136e82b038"
      },
      "source": [
        "# Student Action: Using the 'value_counts()' function, count the number of times 1 and 2 occur in the predicted values.\n",
        "y_predicted.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    570\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfKiAcaMj5xF"
      },
      "source": [
        "As you can see, we did not get the expected results. The model should have classified all the stars having a planet as `2`. Ideally, the Random Forest Classifier model should have classified `565` values as `1` and the remaining `5` values as `2`. \n",
        "\n",
        "In this case, even though the accuracy of a prediction model is high but according to the problem statement, it is not giving the desired result. Hence, **accuracy alone is not the metric to test the efficacy of a prediction model.** \n",
        "\n",
        "In the next class, we will try to investigate why Random Forest Classifier failed to classify even a single star as `2`. Based on the investigation, we will try to improve the model and then deploy it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3WclLnZkFfz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq-eUzqzom6-"
      },
      "source": [
        "### Additional Activities\n",
        "\n",
        "The activities starting from this point are optional. Please do these activities **ONLY** if you have time to spare in the class. Otherwise, skip to the **Wrap-Up** section. The additional activities will not be available in the class copy of the notebook. You will have to manually add these activities in the class copy by adding new text and code cells.\n",
        "\n",
        "Moreover, you don't have to do all the additional activities. Depending on the availability of time in a class, you can choose the number of additional activities to perform from this collection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWYE8EU0syhE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd5jKRKZo7am"
      },
      "source": [
        "#### Activity 1: Multiples\n",
        "\n",
        "Given $n$ and $m$, write a function to print first $m$ multiples of a number $n$ without using any loop. E.g.,\n",
        "\n",
        "```\n",
        "Input: n = 2, m = 3\n",
        "Output: 2 4 6 \n",
        "\n",
        "Input: n = 3, m = 4\n",
        "Output: 3 6 9 12\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSe-CJPypYTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43e123b-f672-42ad-e8ba-492c12702be6"
      },
      "source": [
        "# Solution\n",
        "import numpy as np\n",
        "def m_multiples_of_n(n, m):\n",
        "  multiples = np.arange(n, (n * m) + 1, n)\n",
        "  print(multiples)\n",
        "\n",
        "m_multiples_of_n(2, 3)\n",
        "m_multiples_of_n(3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 6]\n",
            "[ 3  6  9 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fgjzZ1iol77"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh9l4z6r7oM0"
      },
      "source": [
        "#### Activity 2: Centered Penatognal Number\n",
        "\n",
        "Write a function that takes a positive integer and calculates how many dots exist in a pentagonal shape around the centre dot on the $N^{\\text{th}}$ iteration.\n",
        "\n",
        "In the image below you can see the first iteration is only a single dot. On the second, there are 6 dots. On the third, there are 16 dots, and on the fourth, there are 31 dots.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wn1u68GW15KGIz_MWsioUp74B9BPghuP'> \n",
        "\n",
        "So for first 21 positive integers, there should exist 1, 6, 16, 31, 51, 76, 106, 141, 181, 226, 276, 331, 391, 456, 526, 601, 681, 766, 856, 951, 1051 dots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Csrjf01kVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3da3d89-7618-4e13-b0ad-a223a114cfa5"
      },
      "source": [
        "# Solution\n",
        "def centered_pentagonal_dots(num_pentagons):\n",
        "  num = 1\n",
        "  difference = 5\n",
        "  num_pentagons_list = []\n",
        "  while num_pentagons > 0:  \n",
        "    num_pentagons_list.append(num)\n",
        "    num = num + difference\n",
        "    difference += 5\n",
        "    num_pentagons -= 1\n",
        "  # return num_pentagons_list\n",
        "  return num_pentagons_list[-1]\n",
        "\n",
        "print(centered_pentagonal_dots(4))\n",
        "print(centered_pentagonal_dots(10))\n",
        "print(centered_pentagonal_dots(21))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "226\n",
            "1051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRSJsHzHAjtG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUW_JYoQ3Vf-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO5nIhaE3WQl"
      },
      "source": [
        "---"
      ]
    }
  ]
}